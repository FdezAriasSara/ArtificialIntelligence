{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pandas\n",
    "#pip install numpy\n",
    "#pip install -U scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Counter proporciona un diccionario donde cada elemento distinto se almacena \n",
    "# como claves con un valor asociado igual a su cuenta\n",
    "from collections import Counter\n",
    " \n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    " \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOCUMENTACIÓN\n",
    "https://scikit-learn.org/stable/api/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de un modelo con Sklearn\n",
    "\n",
    "En la celda a continuación se resumen las características mas importantes si creamos un modelo a 'mano' con sklearn.\n",
    "ESTO ESTÁ INCLUIDO DENTRO DEL METODO UTILITARIO DE entrenar_y_obtener_accuracy. No usar a menos que manden cambiar parámetros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n{'algorithm': 'auto',\\n 'leaf_size': 30,\\n 'metric': 'minkowski',\\n  'metric_params': None,\\n 'n_jobs': None,\\n 'n_neighbors': 3,\\n 'p': 2, \\n 'weights': 'uniform'}\\n\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# We instantiate a new KNeighborsClassifier object that will represent our model\n",
    "modelo = KNeighborsClassifier(n_neighbors=3)\n",
    "# then , we adjust the model to the training data \n",
    "#modelo.fit(X, y)\n",
    "\n",
    "#Now , we can say our model has been trained. It's time for testing.\n",
    "# We do that by making our model classify unknown data \n",
    "#modelo.predict(X_test)\n",
    "\n",
    "#in case we would like to see the probabilty each element has to belong to one class or another..\n",
    "#modelo.predict_proba(X_test)\n",
    "\n",
    "\n",
    "#results are given inside an array of vectors.\n",
    "\n",
    "#IN ORDER TO SEE WHICH PARAMETERS ARE BEING USED TO TRAIN THE MODEL: \n",
    "#modelo.get_params()\n",
    "\"\"\"\n",
    "{'algorithm': 'auto',\n",
    " 'leaf_size': 30,\n",
    " 'metric': 'minkowski',\n",
    "  'metric_params': None,\n",
    " 'n_jobs': None,\n",
    " 'n_neighbors': 3,\n",
    " 'p': 2, \n",
    " 'weights': 'uniform'}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métodos utilidad - métricas del clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_y_obtener_accuracy(X_train, y_train, X_test, y_test, k=3):\n",
    "    modelo = KNeighborsClassifier(n_neighbors=k)\n",
    "    modelo.fit(X_train, y_train)\n",
    "    y_pred = modelo.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(f'La accuracy es: {accuracy:.2f}')\n",
    "    print('Matriz de confusión:\\n', conf_matrix)\n",
    "    return modelo, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf = pd.read_csv('NOMBREFICHERO')\\nprint(df.info())\\n\\n# Dividir el conjunto de datos en variables (X) y etiquetas (y)\\nX = df.drop(columns=['clase_objetivo'])\\ny = df['clase_objetivo']\\n\\n# Dividir en conjuntos de entrenamiento y test\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n\\n# Definir los trnsformers necesarios\\nnumeric_features = ['variable1', 'variable2']\\ncategorical_features = ['variable_categorica']\\n\\n# Crear el pipeline de preprocesamiento\\n\\n# Para ello creamos un transformer para variables numéricas y otro para variables categóricas\\nnumeric_transformer = Pipeline(steps=[\\n    ('imputer', SimpleImputer(strategy='mean')), \\n    ('scaler', StandardScaler()) \\n])\\n\\ncategorical_transformer = Pipeline(steps=[\\n    ('imputer', SimpleImputer(strategy='most_frequent')),\\n    ('encoder', OneHotEncoder(handle_unknown='ignore'))\\n])\\n\\n#y se lo pasamos a la pipeline de preprocesamiento de datos\\npreprocessor = ColumnTransformer(\\n    transformers=[\\n        ('num', numeric_transformer, numeric_features),\\n        ('cat', categorical_transformer, categorical_features)\\n    ])\\n\\n\\n## USO DEL PIPELINE.\\n\\n# Incluir el clasificador en el pipeline\\nmodelo = KNeighborsClassifier(n_neighbors=3)\\n\\n# Crear el pipeline final  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\npipeline = Pipeline(steps=[\\n    ('preprocessor', preprocessor),\\n    ('classifier', modelo)\\n])\\n\\n\""
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from sklearn.pipeline import Pipeline\n",
    "'''\n",
    "df = pd.read_csv('NOMBREFICHERO')\n",
    "print(df.info())\n",
    "\n",
    "# Dividir el conjunto de datos en variables (X) y etiquetas (y)\n",
    "X = df.drop(columns=['clase_objetivo'])\n",
    "y = df['clase_objetivo']\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Definir los trnsformers necesarios\n",
    "numeric_features = ['variable1', 'variable2']\n",
    "categorical_features = ['variable_categorica']\n",
    "\n",
    "# Crear el pipeline de preprocesamiento\n",
    "\n",
    "# Para ello creamos un transformer para variables numéricas y otro para variables categóricas\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')), \n",
    "    ('scaler', StandardScaler()) \n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "#y se lo pasamos a la pipeline de preprocesamiento de datos\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "\n",
    "## USO DEL PIPELINE.\n",
    "\n",
    "# Incluir el clasificador en el pipeline\n",
    "modelo = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Crear el pipeline final  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', modelo)\n",
    "])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar un conjunto de datos e inizializar las variables correctamente\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento y validación en ficheros distintos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf_train = pd.read_csv('NOMBREFICHERO.csv')  \\nprint(df_train.describe())\\nprint(df_train.shape)\\n# Datos de evaluación - EN CASO DE QUE NOS DEN LOS GRUPOS SEPARADOS\\ndf_test = pd.read_csv('NOMBREFICHERO.csv')\\nX_test = df_test.drop('Clase', axis = 1)\\ny_test = df_test[['Clase']].to_numpy().ravel()\\n\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datos de entrenamiento-EN CASO DE QUE NOS DEN LOS GRUPOS SEPARADOS\n",
    "'''\n",
    "df_train = pd.read_csv('NOMBREFICHERO.csv')  \n",
    "print(df_train.describe())\n",
    "print(df_train.shape)\n",
    "# Datos de evaluación - EN CASO DE QUE NOS DEN LOS GRUPOS SEPARADOS\n",
    "df_test = pd.read_csv('NOMBREFICHERO.csv')\n",
    "X_test = df_test.drop('Clase', axis = 1)\n",
    "y_test = df_test[['Clase']].to_numpy().ravel()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Lo habitual es tener que usar esquemas de validación a la hora de separar los datos. Por lo que nos darán un dataset \"entero\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esquemas de validación\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Holdout validation - DIVISION ENTRENAMIENTO TEST\n",
    "\n",
    "usar este código si nos piden holdout!!! Es importante ya que no se separan primero los conjuntos y luego usamos SOLO el de entrenamiento para el preprocesado, de tal forma que el grupo de validación sea completamente ajeno al proceso de entrenamiento\n",
    "\n",
    "El primero de la práctica usa el conjunto entero para preprocesado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf = pd.read_csv(\\'NOMBREFICHERO.csv\\')\\nprint(df.info())\\nprint(df.describe())\\n\\nX = df.drop(\\'Clase\\', axis = 1)\\ny = df[\\'Clase\\']\\n\\nprint(\"Resultados con semilla 0:\")\\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=0, stratify = y)\\nscaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)\\nentrenar_y_obtener_accuracy(X_train, y_train, X_test, y_test)\\n\\nprint(\"Resultados con semilla 1003:\")\\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=1003, stratify = y)\\nscaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)\\nentrenar_y_obtener_accuracy(X_train, y_train, X_test, y_test)\\n\\nprint(\"Resultados con semilla 19:\")\\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=19, stratify = y)\\nscaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)\\nentrenar_y_obtener_accuracy(X_train, y_train, X_test, y_test)\\n'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "df = pd.read_csv('NOMBREFICHERO.csv')\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "\n",
    "X = df.drop('Clase', axis = 1)\n",
    "y = df['Clase']\n",
    "\n",
    "print(\"Resultados con semilla 0:\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=0, stratify = y)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "entrenar_y_obtener_accuracy(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"Resultados con semilla 1003:\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=1003, stratify = y)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "entrenar_y_obtener_accuracy(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"Resultados con semilla 19:\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=19, stratify = y)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "entrenar_y_obtener_accuracy(X_train, y_train, X_test, y_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### usando un pipeline \n",
    "(HABRÍA QUE CREAR EL PIPELINE CORRESPONDIENTE PRIMERO...  ES POR MOSTRAR QUE SE PASA EL PIPELINE DE FORMA INDEPENDIENTE AL CONJUNTO DE ENTRENAMIENTO,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Entrenar el modelo en el conjunto de datos de entrenamiento\\npipeline.fit(X_train, y_train)\\n\\n# Evaluar el modelo en el conjunto de datos de test\\ny_pred = pipeline.predict(X_test)\\n\\n'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Entrenar el modelo en el conjunto de datos de entrenamiento\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el modelo en el conjunto de datos de test\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scaler- Estratificación\n",
    "Forzar que las particiones sigan la distribución original de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nscaler = StandardScaler()\\nX_scaled = scaler.fit_transform(X)\\n'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Cross validation - Validación cruzada\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORTANTE USAR STRATIFIEDKFOLD PARA QUE SE MANTENGA LA DISTRIBUCIÓN. \n",
    "PASAMOS EL OBJETO QUE DEVUELVE STRATIFIEDKFOLD COMO PARÁMETRO EN EL MÉTODO QUE HACE LA VALIDACIÓN CRUZADA, BAJO EL NOMBRE cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 4 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   variable1            90 non-null     float64\n",
      " 1   variable2            88 non-null     float64\n",
      " 2   variable_categorica  100 non-null    object \n",
      " 3   clase_objetivo       100 non-null    object \n",
      "dtypes: float64(2), object(2)\n",
      "memory usage: 3.3+ KB\n",
      "None\n",
      "Cross-Validation Scores: [0.375  0.5    0.25   0.5    0.1875]\n",
      "Mean Accuracy: 0.36\n",
      "Test Accuracy: 0.35\n",
      "\n",
      "Confusion Matrix - Fold 1:\n",
      " [[19  3  9]\n",
      " [16  4  6]\n",
      " [ 9  8  6]]\n",
      "\n",
      "Confusion Matrix - Fold 2:\n",
      " [[19  3  9]\n",
      " [16  4  6]\n",
      " [ 9  8  6]]\n",
      "\n",
      "Confusion Matrix - Fold 3:\n",
      " [[19  3  9]\n",
      " [16  4  6]\n",
      " [ 9  8  6]]\n",
      "\n",
      "Confusion Matrix - Fold 4:\n",
      " [[19  3  9]\n",
      " [16  4  6]\n",
      " [ 9  8  6]]\n",
      "\n",
      "Confusion Matrix - Fold 5:\n",
      " [[19  3  9]\n",
      " [16  4  6]\n",
      " [ 9  8  6]]\n",
      "\n",
      "Confusion Matrix - Test Set:\n",
      " [[1 3 0]\n",
      " [2 4 1]\n",
      " [4 3 2]]\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "df = pd.read_csv('datos\\datos10.csv')\n",
    "print(df.info())\n",
    "\n",
    "# Dividir el conjunto de datos en variables (X) y etiquetas (y)\n",
    "X = df.drop(columns=['clase_objetivo'])\n",
    "y = df['clase_objetivo']\n",
    "\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Dividir en conjuntos de entrenamiento y test\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "  \n",
    " ################################################### PIPELINE ################################################### \n",
    "# Definir los trnsformers necesarios\n",
    "numeric_features = ['variable1', 'variable2']\n",
    "categorical_features = ['variable_categorica']\n",
    "\n",
    "# Crear el pipeline de preprocesamiento\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[ \n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='mean')), \n",
    "            ('scaler', StandardScaler()) \n",
    "        ]), numeric_features),\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), categorical_features)\n",
    "    ]) \n",
    "# Incluir el clasificador en el pipeline\n",
    "modelo = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Crear el pipeline final\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', modelo)\n",
    "])\n",
    "\n",
    "# Entrenar el modelo en el conjunto de datos de entrenamiento !!!!!!!!!!!!!!!!!! important\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Relizar la validación cruzada usando el pipeline sobre el conjunto de entrenamiento\n",
    "cv_train_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# Ver resultados\n",
    "print(f'Cross-Validation Scores: {cv_train_scores}')\n",
    "print(f'Mean Accuracy: {np.mean(cv_train_scores):.2f}')\n",
    "\n",
    "# Hacer las predicciones en el conjunto de datos de evaluación\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluar los resultados en el conjunto de datos de evaluación\n",
    "accuracy_test = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy_test) \n",
    "\n",
    "########################## MOSTRAR MATRICES DE CONFUSION PARA CADA FOLD ################################################### \n",
    "#from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Realizar la validación cruzada usando cross_val_predict para obtener predicciones en cada iteración\n",
    "y_train_pred_cv = cross_val_predict(pipeline, X_train, y_train, cv=5) #fijate aqui no usa scoring accuracy precisamente para poder ver la matriz entera luego al crear el objeto\n",
    "\n",
    "# Calcular y mostrar la matriz de confusión para cada iteración de la validación cruzada\n",
    "for i in range(5):  # Suponiendo 5-fold cross-validation\n",
    "    cm = confusion_matrix(y_train, y_train_pred_cv)\n",
    "    print(f\"\\nConfusion Matrix - Fold {i + 1}:\\n\", cm)\n",
    "\n",
    "# Hacer predicciones en el conjunto de datos de evaluación\n",
    "y_pred_test = pipeline.predict(X_test)\n",
    "\n",
    "# Calcular y mostrar la matriz de confusión en el conjunto de prueba\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print(\"\\nConfusion Matrix - Test Set:\\n\", cm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESCOGER LOS MEJORES HIPERPARÁMETROS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, en el caso de los vecinos más cercanos, ¿para qué valor de \n",
    " funciona mejor? - LO PODEMOS AUTOMATIZAR CON gridsearch\n",
    "\n",
    "Como los conjuntos de validación durante la validación cruzada se utilizarán para tomar decisiones sobre el modelo, \n",
    "de nuevo corremos el riesgo de que exista data leaking.\n",
    "\n",
    "    PARA EVITAR DATA LEAKING, haremos dos procesos de validación, uno dentro de entrenamiento y otro EXTERNO  \n",
    "    \n",
    "            1- en ENTRENAMIENTO aplicamos CROSS VALIDATION\n",
    "            \n",
    "\n",
    "            2 - tenemos un conjunto de test a parte "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el conjunto de datos\n",
    "df = pd.read_csv('datos09.csv')\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "X = df.drop('Clase', axis = 1)\n",
    "y = df[['Clase']]\n",
    "\n",
    "# Dividir el conjunto de datos en conjuntos de entrenamiento y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definir un diccionario con hiperparámetros\n",
    "param_grid = {'n_neighbors': [3, 7, 11]}\n",
    "\n",
    "# Inicializar el modelo de vecinos más cercanos\n",
    "modelo = KNeighborsClassifier()\n",
    "\n",
    "# Crear un objeto de búsqueda\n",
    "grid_search = GridSearchCV(modelo, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search\n",
    "\n",
    "# Realizar la búsqueda sobre el conjunto de entrenamiento\n",
    "grid_search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Mirar los resultados obtenidos\n",
    "grid_search.cv_results_\n",
    " \n",
    "\n",
    " # Obtener los mejores hiperparámetros y el modelo\n",
    "best_k = grid_search.best_params_['n_neighbors']\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Pronosticar sobre el conjunto de datos de test\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calcular la accuracy y la matriz de confusion\n",
    "accuracy = accuracy_score(y_test, y_pred)    \n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'Mejor valor de k: {best_k}')\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print('Matriz de confusión:\\n', conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# To evaluate the model\\naccuracy = accuracy_score(y_test, y_pred)\\nconf_matrix = confusion_matrix(y_test, y_pred)\\n\\nprint(f'Accuracy: {accuracy:.2f}')\\nprint('Matriz de confusión:\\n', conf_matrix)\\n\""
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "'''\n",
    "# To evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print('Matriz de confusión:\\n', conf_matrix)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
